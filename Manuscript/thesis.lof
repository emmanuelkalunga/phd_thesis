\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces A standard BCI system with signal acquisition, signal processing and application components. The system provides feedback to the user.}}{11}{figure.2.1}
\contentsline {figure}{\numberline {2.2}{\ignorespaces Neuron structure: showing main components of a neuron and its axon-dendrite synaptic connection to a neighbouring neuron \citep {purves_excitatory_2001}}}{12}{figure.2.2}
\contentsline {figure}{\numberline {2.3}{\ignorespaces Electromagnetic activity in pyramidal neuron: (a) EPSPs converge to apical dendrites of the neuron. (b-c) Positive ions enter the neuron and propagate from synapses to the rest of the neuron. (d) the flow of current perpendicular to the apical dendrite is accompanied by a magnetic field that propagates orthogonally. \citep {proverbio_electromagnetic_2003}}}{13}{figure.2.3}
\contentsline {figure}{\numberline {2.4}{\ignorespaces A cut through cortical layers. Electric activities can be measured from different layers. \citep [Reproduced from][]{daly_brain-computer_2008}}}{14}{figure.2.4}
\contentsline {figure}{\numberline {2.5}{\ignorespaces electric signals electrodes: (a) EEG electrodes cap being fitted on a subject's head for recording. (b) ECoG Electrodes. Courtesy of Ripple, Inc. (c) A silicon-based cortical MEA (inset); implanted for intracortical neural recording via a percutaneous connection to a skull mounted pedestal connector \citep {homer_implants_2013}.}}{16}{figure.2.5}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{16}{figure.2.5}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{16}{figure.2.5}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{16}{figure.2.5}
\contentsline {figure}{\numberline {2.6}{\ignorespaces Elekta MEG acquisition system. (a) A MEG shield chamber for electromagnetic isolation (b) MEG sensors configuration. Each sensor location is equipped with three sensors: a magnetometer that measures normal field component, and two orthogonal planar gradiometers that measure gradient components\nobreakspace {}\citep {team_elekta_2016}.}}{18}{figure.2.6}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{18}{figure.2.6}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{18}{figure.2.6}
\contentsline {figure}{\numberline {2.7}{\ignorespaces Trajectory of near-infrared light in the human brain. \citep [Reproduced from][]{gervain_near-infrared_2011}.}}{19}{figure.2.7}
\contentsline {figure}{\numberline {2.8}{\ignorespaces \relax \fontsize {10}{12}\selectfont \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {Standard ERD/ERS-based BCI system paradigm. The break before the next trial should last at least a second to allow the changes in the ongoing EEG/MEG to recover. }}}{24}{figure.2.8}
\contentsline {figure}{\numberline {2.9}{\ignorespaces Sensory homunculus on the left and Motor homunculus on the right. The cortical homunculus initially developed by Dr. Wilder Penfield shows a disproportionate human body laid on the cortex from the prefrontal cortex(top) to the cerebellum (bottom). The size of a given body part of the homunculus is descriptive of the amount of cerebral tissue or cortex devoted to the specific body region which is proportional to how richly innervated that region is. This image is taken from \citep {schoot_penfields_1993} }}{25}{figure.2.9}
\contentsline {figure}{\numberline {2.10}{\ignorespaces \relax \fontsize {10}{12}\selectfont \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {2550 P300 trials have been averaged to obtain the enhanced P300 (blue line). The enhanced P300 is compared to 12750 trials not containing the P300. The data used are subject A's recorded signals from the BCI competition III data set II. A visual oddball paradigm as described in \citep {donchin_mental_2000} is used to elicit the P300.}}}{28}{figure.2.10}
\contentsline {figure}{\numberline {2.11}{\ignorespaces \relax \fontsize {10}{12}\selectfont \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {A P300-speller screen} }}{29}{figure.2.11}
\contentsline {figure}{\numberline {2.12}{\ignorespaces \relax \fontsize {10}{12}\selectfont \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {A Standard Visual Evoked Potential} }}{32}{figure.2.12}
\contentsline {figure}{\numberline {2.13}{\ignorespaces \relax \fontsize {10}{12}\selectfont \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {Visual evoked potentials at stimulation frequency of 2.7 Hz, 4.5 Hz, 7.1 Hz, 12.5 Hz, and 20 Hz. A illustrates the signal in time domain and B illustrates the frequency spectrum. \citep [Reproduced from][]{capilla_steady-state_2011} }. }}{33}{figure.2.13}
\contentsline {figure}{\numberline {2.14}{\ignorespaces \relax \fontsize {10}{12}\selectfont \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {Average of the mean values of the amplitude of the FFT fundamental frequency of the SSVEP recorded at the three occipital leads (Oz, O1, O2) at the different stimulation frequencies. The amplitude of the occipital SSVEP, expressed in microvolts, reached a maximum at 15 Hz and then fell, with a plateau up to 27 Hz, declining at higher frequencies. \citep [Reproduced from][]{pastor_human_2003}.}}}{34}{figure.2.14}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces BCI signal processing pipeline. The fundamental blocks for machine learning are in blue (\textit {i.e.} spatial filtering and classification). Filters and classifiers are learned from training data. Gray blocks (\textit {i.e.} preprocessing and feature representation) do not require learning. }}{41}{figure.3.1}
\contentsline {figure}{\numberline {3.2}{\ignorespaces CSP effect on class distribution. CSP is applied on a 2D toy data set containing samples from two classes marked with red crosses and blue circles. (a) Samples distribution is shown before CSP filtering. The ellipses show estimates of each class covariance. It can be seen that the two classes are highly correlated. The dashed lines show the direction of the CSP projections $\mathbf {w}_j$ ($j=1,2$). (b) Distributions after CSP projections. The two distributions are orthogonal, showing that the two classes are uncorrelated. Each axis gives the largest variance in one class and the smallest in the other. \citep [Image from][]{blankertz_optimizing_2008}.}}{43}{figure.3.2}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{43}{figure.3.2}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{43}{figure.3.2}
\contentsline {figure}{\numberline {3.3}{\ignorespaces \relax \fontsize {10}{12}\selectfont \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {Effect of CSP filtering. A continuous EEG signal containing two right-hand imagery epochs and one left-hand imagery is filtered using four CSP filters ($\mathbf {w}_j$): csp:R1, csp:R2, csp:L1, and csp:L2. The resulting signals from csp:R1 and csp:R2 have large variance during left hand imagery, while signals from csp:L1 and csp:L2 have large variance during right hand imagery. \citep [Image from][]{blankertz_optimizing_2008}.} }}{44}{figure.3.3}
\contentsline {figure}{\numberline {3.4}{\ignorespaces \relax \fontsize {10}{12}\selectfont \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {LDA mapping. The distance $|\mathaccentV {tilde}07E{\ensuremath {\mathbf {m}}}_{(+)}-\mathaccentV {tilde}07E{\ensuremath {\mathbf {m}}}_{(+)}|$} is maximised relative to class variance such as the separability is maximised ($J(\ensuremath {\mathbf {w}})$) }}{46}{figure.3.4}
\contentsline {figure}{\numberline {3.5}{\ignorespaces QDA versus LDA. Data from two subsets shows with red and blue circles are classified with either LDA (in (a) and (c)) or QDA (in (b) and (d)). The black lines show the classifier separation line. Wrongly classified data are shown with squares instead of circles. Class covariances are shown with ellipses of corresponding colours. In (a) and (b), the two subsets have similar covariance, while in (c) and (d) they have different covariance \citep {scikit-learn}}}{47}{figure.3.5}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{47}{figure.3.5}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{47}{figure.3.5}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{47}{figure.3.5}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{47}{figure.3.5}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Examples of SVM classifiers. SVM is applied on 2D artificial data forming two classes represented in red and blue. The hyperplane $\left \delimiter "426830A \ensuremath {x}_i, \ensuremath {\mathbf {w}}\right \delimiter "526930B + b = 0$ separating the two classes is shown, as well as the margins$\left \delimiter "426830A \ensuremath {x}_i, \ensuremath {\mathbf {w}}\right \delimiter "526930B + b \geq \frac {1}{||\ensuremath {\mathbf {w}}||}$. Only few samples relatively close to the hyperplanes are used as support vectors; they are shown with big circles \citep {scikit-learn}.}}{50}{figure.3.6}
\contentsline {figure}{\numberline {3.7}{\ignorespaces Multiclass SVM classification with different kernels on 2D projection of the iris dataset. The decision surface separating three classes are shown. The x-axis and y-axis represent sepal length and sepal width respectively. (a) Linear kernel, (b) polynomial kernel of order 3, (c) RBF kernel \citep {scikit-learn}.}}{51}{figure.3.7}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{51}{figure.3.7}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{51}{figure.3.7}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{51}{figure.3.7}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Adopted machine learning pipeline. It consists of 3 main phases: 1) Offline model selection: A number of preprocessing and classifiers are tested on a collection of data from multiple subjects, the best combination is selected. 2) Training phase: Classifier parameters are trained for each new subject before classification. 3) Classification: A new [unseen] sample is preprocessed similarly to training data, then classified.}}{63}{figure.3.8}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Hybrid BCI system integration. The motor abilities of the user are the primary controller of the system, using adapted interfaces (here: 3D touchless interface). The brain-computer interface (here: SSVEP) provides a complementary communication channel and is designed to trigger specific actions.}}{66}{figure.4.1}
\contentsline {figure}{\numberline {4.2}{\ignorespaces The 3D touchless interface and a user's hand positions for different commands: (a) left, (b) right, (c) up, (d) down, (e) forward, (f) backward, (g) rest. The IR-sensors are in the black plastic housing on the right side of the hand and around the wrist. Another symmetrical plastic housing has been realised for left-handed users.}}{67}{figure.4.2}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{67}{figure.4.2}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{67}{figure.4.2}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{67}{figure.4.2}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{67}{figure.4.2}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {}}}{67}{figure.4.2}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {}}}{67}{figure.4.2}
\contentsline {subfigure}{\numberline {(g)}{\ignorespaces {}}}{67}{figure.4.2}
\contentsline {figure}{\numberline {4.3}{\ignorespaces \relax \fontsize {10}{12}\selectfont \abovedisplayskip 10\p@ plus2\p@ minus5\p@ \abovedisplayshortskip \z@ plus3\p@ \belowdisplayshortskip 6\p@ plus3\p@ minus3\p@ \def \leftmargin \leftmargini \parsep 5\p@ plus2.5\p@ minus\p@ \topsep 10\p@ plus4\p@ minus6\p@ \itemsep 5\p@ plus2.5\p@ minus\p@ {\leftmargin \leftmargini \topsep 6\p@ plus2\p@ minus2\p@ \parsep 3\p@ plus2\p@ minus\p@ \itemsep \parsep }\belowdisplayskip \abovedisplayskip {Acquisition material, the EEG is recorded with electrodes, the signal is amplified and sent to a computer running OpenVIBE.}}}{68}{figure.4.3}
\contentsline {figure}{\numberline {4.4}{\ignorespaces Experimental setting: subject sitting on an electric wheelchair equipped with a robotic arm exoskeleton. During offline recordings, the exoskeleton and the touchless interface are disabled; the subject performs the SSVEP task as prompted by audio cues.}}{69}{figure.4.4}
\contentsline {figure}{\numberline {4.5}{\ignorespaces Evaluation of the online performances of the proposed BCI algorithm. The error rates for all five subjects are indicated as a function of time, with $t$+0 indicating the first prediction made (after $\ensuremath {t_W}= 3$\nobreakspace {}s). The error rates are averaged on all classes.}}{73}{figure.4.5}
\contentsline {figure}{\numberline {4.6}{\ignorespaces Assessment of the accuracy of classification depending on the time of the prediction. On x-axis, $t$+0 indicates the first prediction made three seconds after the start of a trial. The results are averaged on all trials for each subject. Subject 1 is the only one to present a slight increase of the classification accuracy. For all other subjects, the algorithm proposes a correct answer as the first prediction.}}{74}{figure.4.6}
\contentsline {figure}{\numberline {4.7}{\ignorespaces Experiment in the virtual environment. Here the subject is using the 3D touchless interface with his right hand and the SSVEP LEDs are put in front of him. The screen displays a helicopter in the virtual environment. The subject should pass through all waypoints, materialised by red (or grey) disks on the screen. When the subject triggers a shortcut, the helicopter is automatically moved to a location materialised by the transparent ball.}}{75}{figure.4.7}
\contentsline {figure}{\numberline {4.8}{\ignorespaces Subject sitting on the ESTA wheelchair. His arm is supported by the exoskeleton, and the left hand is lying on the touchless interface. On his right-hand side is the SSVEP stimulation board. He is fitted with an EEG cap for brain signals recording. Next to the exoskeleton, an object is put on a table. (a) The subject is in resting position. He is gazing at the 17 Hz LED to trigger an automatic trajectory to the table. (b) Subject has reached the table and is using the touchless interface to reach and grab the glass (c) Glass in hand, the user gazing at the 13 Hz LED to activate the automatic trajectory to mouth. (d) The arm reaches the mouth while the touchless interface is deactivated.}}{77}{figure.4.8}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{77}{figure.4.8}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{77}{figure.4.8}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{77}{figure.4.8}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{77}{figure.4.8}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Geometry of the Bregman divergence with the seed function $f(z)=\frac {1}{2}z^\intercal z$. $h(z)$ is a hyperplane tangent to $f(z)$ at $y$. While it accurately represents $f(y)$, it underestimates $f(x)$. The Bregman divergence measures how much the representation of $f(x)$ on $h(z)$ \emph {diverges} from $f(x)$ (in green).}}{90}{figure.5.1}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Comparison of covariance estimators in terms of classification accuracy obtained with MDRM with increasing EEG trial length. For each trial length, the average accuracy across all subjects and across all replication is shown. Bars indicate the error of the mean, i.e. standard deviation divided by the square root of $n-1$, $n$ = number of samples.}}{100}{figure.5.2}
\contentsline {figure}{\numberline {5.3}{\ignorespaces (a) Covariance matrices condition expressed as the ratio $\mathcal {C}$ between largest and smallest eigenvalues for the different covariance estimators. The comparison is made for increasing EEG trial length. (b) Integrated discrimination improvement brought to the classification task by various estimators along varying trail length. The indicated IDI values are multiplied by $10^{2}$. $\ensuremath {\mathaccentV {hat}05E{\Sigma }_{\operatorname {scm}}}$ is used as a baseline. }}{102}{figure.5.3}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{102}{figure.5.3}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{102}{figure.5.3}
\contentsline {figure}{\numberline {5.4}{\ignorespaces Scatter plot of covariance matrices for all trials mapped on the tangent space. The distance between each trial covariance matrix $\ensuremath {\Sigma }_{\ensuremath {i}}$ and its Riemannian mean class $\ensuremath {\Sigma }_{\ensuremath {\mu }}^{(\ensuremath {k})}$ is shown as connection line. The black star represents the Riemannian mean of all trials. Subject with lowest BCI performance, (\ref {fig:tan_plan_s16_a}) before and (\ref {fig:tan_plan_s16_b}) after Riemannian potato filtering. Subject with highest BCI performance, (\ref {fig:tan_plan_s17_a}) before and (\ref {fig:tan_plan_s17_b}) after Riemannian potato filtering.}}{104}{figure.5.4}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{104}{figure.5.4}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{104}{figure.5.4}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{104}{figure.5.4}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{104}{figure.5.4}
\contentsline {figure}{\numberline {5.5}{\ignorespaces Representation of covariance matrices: each image is the covariance matrix mean $\ensuremath {\mathaccentV {bar}016{\ensuremath {\Sigma }}}^{(\ensuremath {k})}$ of the class $\ensuremath {k}$, for one session of the recording. The diagonal blocks show the covariance in different frequency bands, i.e. 13 Hz in the upper-left block, 21 Hz in the middle, and 17 Hz in the bottom-right. Subjects with highest (a) and lowest (b) BCI performance. }}{105}{figure.5.5}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{105}{figure.5.5}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{105}{figure.5.5}
\contentsline {figure}{\numberline {5.6}{\ignorespaces (a): Swelling effect of Arithmetic mean shown through log-determinant values. Training trials are taken from the 13 Hz class of the subject with the highest BCI performance. Log-determinant values are given for each trial covariance (points), and for means of Table\nobreakspace {}\ref {tab:dist} (horizontal lines). (b): Classification accuracy and CPU time, obtained with $\alpha $-divergence for $-1\leqslant \alpha \leqslant 1$.}}{106}{figure.5.6}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{106}{figure.5.6}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{106}{figure.5.6}
\contentsline {figure}{\numberline {5.7}{\ignorespaces Signal amplitude at each stimulus frequency, showing synchronisation of EEG with respect to time (seconds). The raw signal of the trial measured on Oz is band filtered using a Butterworth of order 8 at each stimulus frequency and the resulting signals are shown in blue (dark grey), green (grey), and red (light grey) for the same signal filtered respectively at 13, 17, and 21 Hz. The cue onset $\ensuremath {\tau _0}$ at time $0$ on the x-axis is shown with a vertical discontinued line. 4 trials are shown, one for each class. Signals are from the subjects with the highest (\ref {fig:sync_delay_best}) and with the lowest BCI performance (\ref {fig:sync_delay_worst}).}}{108}{figure.5.7}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{108}{figure.5.7}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{108}{figure.5.7}
\contentsline {figure}{\numberline {5.8}{\ignorespaces Evaluation of the online algorithm parameters. \ref {fig:classErrorEpochs} shows the decrease of the average classification error over all subjects during the successive epochs after the beginning of the trial. \ref {fig:classProbEpochs} is an example taken from the subject with the best performance showing how the probability of the actual class varies with epoch position from beginning of trial. The groundtruth class probability is represented with a thick-and-star line, while other classes probability lines are thin-and-diamond. \ref {fig:errorProbTresh} shows the variation of the average classification error for different probability threshold ($0 \leqslant \ensuremath {\vartheta }< 1$) and its influence on the classifier output (Algorithm \ref {alg:online} step \ref {op3:test_rho}). \ref {fig:accaracy_tlen} shows how the average online performance varies with respect to the epoch size ($\ensuremath {w}$). It shows both the classification accuracy (left y-axis) and the ITR (right y-axis). In \ref {fig:classErrorEpochs}, \ref {fig:errorProbTresh}, and \ref {fig:accaracy_tlen}, the bars represent the error of the mean i.e. standard deviation divided by the square root of $n-1$, $n$ = number of samples.}}{116}{figure.5.8}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{116}{figure.5.8}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{116}{figure.5.8}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{116}{figure.5.8}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{116}{figure.5.8}
\contentsline {figure}{\numberline {5.9}{\ignorespaces The covariance matrices trajectory during a 4-class SSVEP online recording. The circles represent class centres. The triangles mark the beginning of the experiment of a new trial whose class is indicated by the triangle's colour. \ref {fig:class_path1_s17} shows the first 7 trials. The first 3 trials are from the resting class, the remaining are respectively class 13 Hz, 17 Hz, and 21 Hz. \ref {fig:class_path2_s17} shows the entire recording. Data are taken from the subject with the highest BCI performance.}}{117}{figure.5.9}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{117}{figure.5.9}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{117}{figure.5.9}
\contentsline {figure}{\numberline {5.10}{\ignorespaces (a) Confusion matrix for $K=4$ classes with Online-Potato. (b): ROC curve indicating the influence of the $\ensuremath {\vartheta }$ parameter. }}{118}{figure.5.10}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{118}{figure.5.10}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{118}{figure.5.10}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces Hand-written digits from MNIST dataset. The original data are on the first row, the other rows are artificially created images from distorted version of the original digits. \citep [Image taken from ][]{ciresan_multi-column_2012}}}{120}{figure.6.1}
\contentsline {figure}{\numberline {6.2}{\ignorespaces Mapping of covariance matrices of trials from each class on the tangent space \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:log_r}\unskip \@@italiccorr )}}. Matrices on the tangent space are vectorised and the 2 most significant components from PCA are used to obtain the 2-D representation. The covariance matrices of original data (a) and augmented data (b).}}{125}{figure.6.2}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{125}{figure.6.2}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{125}{figure.6.2}
\contentsline {figure}{\numberline {6.3}{\ignorespaces Mean classification accuracy in \% across all subjects for different levels of data augmentation. At 0, there is no augmented data. At 1, one artificial data is interpolated between each pair of original data within each class, and so forth}}{126}{figure.6.3}
\contentsline {figure}{\numberline {6.4}{\ignorespaces Classification accuracy of subject with lowest BCI performance versus subject with highest BCI performance, using original training set and using augmented training set with 5 interpolated points between each pair of original data within each class.}}{127}{figure.6.4}
\contentsline {figure}{\numberline {6.5}{\ignorespaces Classification performance in terms of sensitivity. For each of the 16 subjects these measures are given for classification based on training on original unbalance training set and training on augmented and balanced training set. }}{129}{figure.6.5}
\contentsline {figure}{\numberline {6.6}{\ignorespaces 2-D representation of affinity (or similarity) between subjects based on the 4 metrics: \ref {fig:subjects_similarity_AIR}: Affine Invariant Riemannian distance, \ref {fig:subjects_similarity_KL_up}: Kullback-Leibler using forward divergences, \ref {fig:subjects_similarity_KL_low}: Kullback-Leibler using reverse divergences, \ref {fig:subjects_similarity_KL_bar}: the symmetric version of KL divergence.}}{133}{figure.6.6}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{133}{figure.6.6}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{133}{figure.6.6}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{133}{figure.6.6}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{133}{figure.6.6}
\contentsline {figure}{\numberline {6.7}{\ignorespaces Mean classification accuracy for 12 subjects. Grid search with different values of $n \in \left \delimiter "4266308 4, 8, 12, 16, 20, 24, 28, 32 \right \delimiter "5267309 $ on the x-axis and different values of $\lambda \in \left \delimiter "4266308 0, 0.2, 0.4, 0.6, 0.8, 1 \right \delimiter "5267309 $ on the y-axis. The results are obtained using 4 metrics to measure similarity between subjects: \ref {fig:surfmean_airm} AIRM, \ref {fig:surfmean_kld_for} $\ensuremath {D_{\operatorname {KL}}}^{forward}$, \ref {fig:surfmean_kld_rev} $\ensuremath {D_{\operatorname {KL}}}^{reverse}$, \ref {fig:surfmean_kld_sym} $\ensuremath {D_{\operatorname {KL}}}^{symmetric}$}}{134}{figure.6.7}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{134}{figure.6.7}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{134}{figure.6.7}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{134}{figure.6.7}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{134}{figure.6.7}
\contentsline {figure}{\numberline {6.8}{\ignorespaces Individual subject classification accuracy. Grid search with different values of $n \in \left \delimiter "4266308 4, 8, 12, 16, 20, 24, 28, 32 \right \delimiter "5267309 $ on the x-axis and different values of $\lambda \in \left \delimiter "4266308 0, 0.2, 0.4, 0.6, 0.8, 1 \right \delimiter "5267309 $ on the y-axis. (a) to (l) correspond to subjects 1 to 12 respectively. The results are obtained using $\ensuremath {D_{\operatorname {KL}}}^{symmetric}$}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(a)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(b)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(c)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(d)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(e)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(f)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(g)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(h)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(i)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(j)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(k)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {subfigure}{\numberline {(l)}{\ignorespaces {}}}{135}{figure.6.8}
\contentsline {figure}{\numberline {6.9}{\ignorespaces Optimal performance (Pareto front) obtained through a grid search is compared with the performance with $\lambda = 0$, $\lambda = 1$, and $\lambda $ as proposed in \textup {\hbox {\mathsurround \z@ \normalfont (\ignorespaces \ref {eq:lambda}\unskip \@@italiccorr )}}.}}{136}{figure.6.9}
\addvspace {10\p@ }
